{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load Fast-text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_home = './'\n",
    "words_to_load = 50000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with open(ft_home + 'wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load+2, 300), dtype=np.float32)\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i+2, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i+2\n",
    "        idx2words_ft[i+2] = s[0]\n",
    "        ordered_words_ft.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add padding and unknow token\n",
    "words_ft['<PAD>'] = 0\n",
    "words_ft['<UNK>'] = 1\n",
    "idx2words_ft[0] = '<PAD>'\n",
    "idx2words_ft[1] = '<UNK>'\n",
    "#init padding and unknow embedding to gaussian random numbers\n",
    "loaded_embeddings_ft[0,:] = np.zeros(loaded_embeddings_ft.shape[1])\n",
    "loaded_embeddings_ft[1,:] = np.random.normal(size = (loaded_embeddings_ft.shape[1],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save embedding matrix\n",
    "# pk.dump(torch.from_numpy(loaded_embeddings_ft), open('./hw2_data/loaded_embeddings_ft.pk', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset, label_dct, word2id, id2word):\n",
    "    dataset['sentence1'] = dataset['sentence1'].apply(lambda x: x.split()) \n",
    "    dataset['sentence2'] = dataset['sentence2'].apply(lambda x: x.split()) \n",
    "    dataset['label'] = dataset['label'].apply(lambda x: label_dct[x])\n",
    "    \n",
    "    def token2vocab(row):\n",
    "        for i in range(len(row)):\n",
    "            if row[i] in word2id.keys(): \n",
    "                row[i] = word2id[row[i]]\n",
    "            else: row[i] = word2id['<UNK>']\n",
    "        return row\n",
    "\n",
    "    dataset['sentence1'] = dataset['sentence1'].apply(lambda x: token2vocab(x))\n",
    "    dataset['sentence2'] = dataset['sentence2'].apply(lambda x: token2vocab(x))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dct = {'neutral':0, 'entailment':1, 'contradiction':2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train = pd.read_csv('hw2_data/snli_train.tsv', sep = '\\t')\n",
    "snli_val = pd.read_csv('hw2_data/snli_val.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train_id = preprocess(snli_train, label_dct, words_ft, idx2words_ft)\n",
    "snli_val_id = preprocess(snli_val, label_dct, words_ft, idx2words_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pk.dump(snli_train_id, open(\"./hw2_data/snli_train_id.pk\", \"wb\"))\n",
    "# pk.dump(snli_val_id, open(\"./hw2_data/snli_val_id.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train = pd.read_csv('hw2_data/mnli_train.tsv', sep = '\\t')\n",
    "mnli_val = pd.read_csv('hw2_data/mnli_val.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Genres = mnli_train['genre'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train_dict = {}\n",
    "mnli_val_dict = {}\n",
    "for g in Genres:\n",
    "    mnli_train_dict[g] = mnli_train[mnli_train['genre'] == g].drop('genre', axis = 1)\n",
    "    mnli_train_dict[g] = preprocess(mnli_train_dict[g], label_dct, words_ft, idx2words_ft)\n",
    "    mnli_val_dict[g] = mnli_val[mnli_val['genre']==g].drop('genre', axis = 1)\n",
    "    mnli_val_dict[g] = preprocess(mnli_val_dict[g], label_dct, words_ft, idx2words_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pk.dump(mnli_train_dict, open(\"./hw2_data/mnli_train_dict.pk\", \"wb\"))\n",
    "# pk.dump(mnli_val_dict, open(\"./hw2_data/mnli_val_dict.pk\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
